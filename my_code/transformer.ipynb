{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T13:40:02.454779Z",
     "start_time": "2024-12-31T13:39:56.684346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 加载IMDB数据集，num_words=10000表示使用前10000个最常用的词\n",
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words=10000)\n",
    "\n",
    "# 创建反向映射\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "index_word = {index: word for word, index in word_index.items()}\n",
    "\n",
    "# 将每个评论的整数列表转换为单词列表\n",
    "train_texts = [' '.join([index_word.get(i - 3, '?') for i in review]) for review in train_data]\n",
    "test_texts = [' '.join([index_word.get(i - 3, '?') for i in review]) for review in test_data]\n",
    "\n",
    "# 合并训练集和测试集文本\n",
    "texts = train_texts + test_texts\n",
    "labels = np.array(train_labels.tolist() + test_labels.tolist())\n"
   ],
   "id": "c7e725698f294f7c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T13:40:05.308714Z",
     "start_time": "2024-12-31T13:40:03.316747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# 下载 NLTK 停用词库（如果尚未下载）\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 获取英文停用词\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 定义文本清理函数\n",
    "def clean_text(text):\n",
    "    # 转小写\n",
    "    text = text.lower()\n",
    "    # 去除标点符号和非字母字符\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # 按空格分词\n",
    "    words = text.split()\n",
    "    # 去除停用词\n",
    "    cleaned_words = [word for word in words if word not in stop_words]\n",
    "    # 重新组合为字符串\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# 清理文本\n",
    "cleaned_texts = [clean_text(text) for text in texts]\n"
   ],
   "id": "ea5db8792f20979",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T13:40:18.552624Z",
     "start_time": "2024-12-31T13:40:06.593422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "\n",
    "# 保存清理后的文本到文件\n",
    "with open(\"../txt/cleaned_texts.txt\", 'w') as f:\n",
    "    for text in cleaned_texts:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "# 训练Word2Vec模型\n",
    "word2vec_model = Word2Vec(LineSentence('../txt/cleaned_texts.txt'), vector_size=50, window=10, min_count=1, workers=4)\n",
    "word2vec_model.save(\"word2vec_50dim.model\")\n",
    "\n",
    "# 查看词向量示例\n",
    "print(word2vec_model.wv['good'])\n"
   ],
   "id": "94d0660318d7555c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.5568116  -0.5148969  -0.30211067 -0.6525784  -0.7846254  -0.07693809\n",
      "  2.7421577  -3.8669763   3.233074   -0.05970954 -0.15017028  0.16839494\n",
      "  1.3168415  -1.35753     1.1577648   1.1919928  -1.2399462  -0.38931683\n",
      " -0.58445    -0.08187722 -2.61016     1.9360516  -1.0797453   0.2688658\n",
      " -1.2063103  -0.5822842   1.4312816   4.364593    0.5273557   0.0417451\n",
      " -1.8826184  -1.4872185  -0.37934548 -0.08250141  1.116712   -1.1693203\n",
      " -1.6756998  -2.7791555   0.55821395  0.85290354 -1.491963    5.5283957\n",
      "  1.3680674  -1.787526   -2.5441642  -1.9117925  -1.3347747   2.600239\n",
      "  1.1327598  -5.540544  ]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T13:40:26.902268Z",
     "start_time": "2024-12-31T13:40:20.095779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split  # 添加这个导入\n",
    "\n",
    "# 检查是否可以使用 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 假设 word2vec_model 已经加载好\n",
    "embedding_size = 50  # 假设使用50维的词向量\n",
    "sequence_length = 100  # 统一句子长度为100\n",
    "\n",
    "# 将文本转化为整数索引\n",
    "def text_to_indices(text, model, sequence_length=100):\n",
    "    sentence_indices = []\n",
    "    for word in text.split():\n",
    "        if word in model.wv:\n",
    "            # 获取词的索引\n",
    "            sentence_indices.append(model.wv.key_to_index[word])\n",
    "        else:\n",
    "            sentence_indices.append(0)  # 如果词不在词汇表中，使用0（通常代表未知词）\n",
    "    \n",
    "    # 填充或截断到固定长度\n",
    "    if len(sentence_indices) < sequence_length:\n",
    "        sentence_indices.extend([0] * (sequence_length - len(sentence_indices)))\n",
    "    else:\n",
    "        sentence_indices = sentence_indices[:sequence_length]\n",
    "    \n",
    "    return np.array(sentence_indices)\n",
    "\n",
    "# 将所有文本转换为索引\n",
    "inputs = np.array([text_to_indices(text, word2vec_model, sequence_length) for text in cleaned_texts])\n",
    "\n",
    "# 假设labels已经定义\n",
    "inputs_tensor = torch.LongTensor(inputs).to(device)\n",
    "labels_tensor = torch.LongTensor(labels).to(device)\n",
    "\n",
    "# 检查文本和标签的数量是否一致\n",
    "print(len(inputs_tensor) == len(labels_tensor))  # 确保数据和标签长度一致\n",
    "\n",
    "# 划分训练集和验证集\n",
    "X_train, X_val, y_train, y_val = train_test_split(inputs_tensor, labels_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ],
   "id": "706c9539bfe3466d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T13:40:27.746648Z",
     "start_time": "2024-12-31T13:40:27.736625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, embedding_size, num_classes, num_heads, num_layers, hidden_dim, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        # 将 NumPy 数组转换为 PyTorch tensor\n",
    "        word2vec_tensor = torch.tensor(word2vec_model.wv.vectors, dtype=torch.float32).to(device)\n",
    "\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding.from_pretrained(word2vec_tensor, freeze=False)  # 使用预训练词向量\n",
    "\n",
    "        # Transformer模型，设置batch_first=True\n",
    "        self.transformer = nn.Transformer(d_model=embedding_size, nhead=num_heads,\n",
    "                                          num_encoder_layers=num_layers, dim_feedforward=hidden_dim,\n",
    "                                          dropout=dropout, batch_first=True)\n",
    "        # 分类头\n",
    "        self.fc = nn.Linear(embedding_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: (batch_size, seq_len)\n",
    "        x = self.embedding(x)  # 将输入转为嵌入，输出形状为 (batch_size, seq_len, embedding_size)\n",
    "\n",
    "        # 检查 Transformer 输入的维度是否正确\n",
    "        assert x.size(-1) == self.transformer.d_model, f\"Input dimension {x.size(-1)} does not match d_model {self.transformer.d_model}\"\n",
    "\n",
    "        # Transformer模型\n",
    "        # 注意：这里的输入 x 和目标 x 是相同的，符合自编码器结构\n",
    "        transformer_output = self.transformer(x, x)  # 使用自编码器结构，源和目标相同\n",
    "\n",
    "        # 取Transformer输出的最后一个时间步的输出\n",
    "        x = transformer_output[:, -1, :]  # 形状变为 (batch_size, embedding_size)\n",
    "\n",
    "        # 分类层\n",
    "        x = self.fc(x)  # 直接通过分类层\n",
    "\n",
    "        return x"
   ],
   "id": "943cd6172183948a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T13:44:27.322287Z",
     "start_time": "2024-12-31T13:44:27.300295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 训练并验证函数\n",
    "def train_and_validate(model, train_loader, val_loader, epochs=10, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)  # 学习率衰减\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # 将输入和标签移到设备上\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # 前向传播\n",
    "            loss = loss_fn(outputs, labels)  # 计算损失\n",
    "            loss.backward()  # 反向传播\n",
    "            optimizer.step()  # 更新参数\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}, Val Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        scheduler.step()  # 更新学习率"
   ],
   "id": "427b622ff5520e75",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T13:53:49.852994Z",
     "start_time": "2024-12-31T13:44:28.044895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 选择模型并开始训练\n",
    "transformer_model = TransformerModel(\n",
    "    embedding_size=embedding_size,\n",
    "    num_classes=2,\n",
    "    num_heads=10,\n",
    "    num_layers=2,\n",
    "    hidden_dim=256\n",
    ").to(device)\n",
    "\n",
    "train_and_validate(transformer_model, train_loader, val_loader, epochs=10, lr=0.001)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(transformer_model.state_dict(), 'model/transformer_model.pth')"
   ],
   "id": "f339bfb4d2e1a523",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.3572, Val Accuracy: 86.42%\n",
      "Epoch [2/10], Loss: 0.2975, Val Accuracy: 86.95%\n",
      "Epoch [3/10], Loss: 0.2585, Val Accuracy: 87.38%\n",
      "Epoch [4/10], Loss: 0.2026, Val Accuracy: 87.40%\n",
      "Epoch [5/10], Loss: 0.1883, Val Accuracy: 87.31%\n",
      "Epoch [6/10], Loss: 0.1800, Val Accuracy: 87.06%\n",
      "Epoch [7/10], Loss: 0.1713, Val Accuracy: 87.12%\n",
      "Epoch [8/10], Loss: 0.1690, Val Accuracy: 87.14%\n",
      "Epoch [9/10], Loss: 0.1687, Val Accuracy: 87.02%\n",
      "Epoch [10/10], Loss: 0.1678, Val Accuracy: 87.04%\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory model does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 13\u001B[0m\n\u001B[0;32m     10\u001B[0m train_and_validate(transformer_model, train_loader, val_loader, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# 保存模型\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformer_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel/transformer_model.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\binbin\\app\\conda\\envs\\Pytorch1\\lib\\site-packages\\torch\\serialization.py:651\u001B[0m, in \u001B[0;36msave\u001B[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001B[0m\n\u001B[0;32m    648\u001B[0m _check_save_filelike(f)\n\u001B[0;32m    650\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n\u001B[1;32m--> 651\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_zipfile_writer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[0;32m    652\u001B[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001B[0;32m    653\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[1;32mF:\\binbin\\app\\conda\\envs\\Pytorch1\\lib\\site-packages\\torch\\serialization.py:525\u001B[0m, in \u001B[0;36m_open_zipfile_writer\u001B[1;34m(name_or_buffer)\u001B[0m\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    524\u001B[0m     container \u001B[38;5;241m=\u001B[39m _open_zipfile_writer_buffer\n\u001B[1;32m--> 525\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcontainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\binbin\\app\\conda\\envs\\Pytorch1\\lib\\site-packages\\torch\\serialization.py:496\u001B[0m, in \u001B[0;36m_open_zipfile_writer_file.__init__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m    494\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39mPyTorchFileWriter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile_stream))\n\u001B[0;32m    495\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 496\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPyTorchFileWriter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Parent directory model does not exist."
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T14:25:40.656951Z",
     "start_time": "2024-12-31T14:25:36.692397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载训练好的Transformer模型\n",
    "transformer_model.load_state_dict(torch.load('../model/transformer_model.pth', weights_only=True))\n",
    "transformer_model.eval()  # 切换为评估模式\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # 不需要计算梯度\n",
    "    for batch in val_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # 确保输入和标签在正确的设备上\n",
    "        outputs = transformer_model(inputs)  # 获取模型输出\n",
    "\n",
    "        # 获取预测类别\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        print(predicted.shape)\n",
    "        # 统计正确预测的数量\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n"
   ],
   "id": "7e98a86fc1d26c7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([16])\n",
      "Accuracy: 87.38%\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "312dab892bffc9a5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
