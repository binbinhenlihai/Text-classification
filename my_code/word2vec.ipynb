{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.加载数据集",
   "id": "bda5811a5aa33ee5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T09:20:39.088052Z",
     "start_time": "2025-01-03T09:20:21.343630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os as os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "datapath = r'../IMDB数据集'\n",
    "save_dir = r'../imbd'\n",
    "\n",
    "def get_data(datapath):\n",
    "    pos_files = os.listdir(datapath + '/pos')\n",
    "    neg_files = os.listdir(datapath + '/neg')\n",
    "    print(len(pos_files))\n",
    "    print(len(neg_files))\n",
    "\n",
    "    pos_all = []\n",
    "    neg_all = []\n",
    "    for pf, nf in zip(pos_files, neg_files):\n",
    "        with open(datapath + '/pos' + '/' + pf, encoding='utf-8') as f:\n",
    "            s = f.read()\n",
    "            pos_all.append(s)\n",
    "        with open(datapath + '/neg' + '/' + nf, encoding='utf-8') as f:\n",
    "            s = f.read()\n",
    "            neg_all.append(s)\n",
    "\n",
    "    X_orig= np.array(pos_all + neg_all)\n",
    "    Y_orig = np.array([1 for _ in range(len(pos_all))] + [0 for _ in range(len(neg_all))])\n",
    "    print(\"X_orig:\", X_orig.shape)\n",
    "    print(\"Y_orig:\", Y_orig.shape)\n",
    "\n",
    "    return X_orig, Y_orig\n",
    "\n",
    "def generate_train_data():\n",
    "    X_orig, Y_orig = get_data(datapath+r'/train')\n",
    "    X_test, Y__test = get_data(datapath+r'/test')\n",
    "    X = np.concatenate([X_orig, X_test])\n",
    "    Y = np.concatenate([Y_orig, Y__test])\n",
    "    np.random.seed = 1\n",
    "    random_indexs = np.random.permutation(len(X))\n",
    "    X = X[random_indexs]\n",
    "    Y = Y[random_indexs]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)\n",
    "    print(\"X_train:\", X_train.shape)\n",
    "    print(\"y_train:\", y_train.shape)\n",
    "    print(\"X_test:\", X_test.shape)\n",
    "    print(\"y_test:\", y_test.shape)\n",
    "    print(\"x_val:\", X_val.shape)\n",
    "    print(\"y_val:\", y_val.shape)\n",
    "    np.savez(save_dir + '/imdb_train', x=X_train, y=y_train)\n",
    "    np.savez(save_dir + '/imdb_test', x=X_test, y=y_test)\n",
    "    np.savez(save_dir + '/imdb_val', x=X_val, y=y_val)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    generate_train_data()\n"
   ],
   "id": "4c61d42923a340b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n",
      "X_orig: (25000,)\n",
      "Y_orig: (25000,)\n",
      "12500\n",
      "12500\n",
      "X_orig: (25000,)\n",
      "Y_orig: (25000,)\n",
      "X_train: (36000,)\n",
      "y_train: (36000,)\n",
      "X_test: (10000,)\n",
      "y_test: (10000,)\n",
      "x_val: (4000,)\n",
      "y_val: (4000,)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../imbd/imdb_train.npz'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 53\u001B[0m\n\u001B[0;32m     50\u001B[0m     np\u001B[38;5;241m.\u001B[39msavez(save_dir \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/imdb_val\u001B[39m\u001B[38;5;124m'\u001B[39m, x\u001B[38;5;241m=\u001B[39mX_val, y\u001B[38;5;241m=\u001B[39my_val)\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m---> 53\u001B[0m     \u001B[43mgenerate_train_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[1], line 48\u001B[0m, in \u001B[0;36mgenerate_train_data\u001B[1;34m()\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx_val:\u001B[39m\u001B[38;5;124m\"\u001B[39m, X_val\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_val:\u001B[39m\u001B[38;5;124m\"\u001B[39m, y_val\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m---> 48\u001B[0m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msavez\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/imdb_train\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m np\u001B[38;5;241m.\u001B[39msavez(save_dir \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/imdb_test\u001B[39m\u001B[38;5;124m'\u001B[39m, x\u001B[38;5;241m=\u001B[39mX_test, y\u001B[38;5;241m=\u001B[39my_test)\n\u001B[0;32m     50\u001B[0m np\u001B[38;5;241m.\u001B[39msavez(save_dir \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/imdb_val\u001B[39m\u001B[38;5;124m'\u001B[39m, x\u001B[38;5;241m=\u001B[39mX_val, y\u001B[38;5;241m=\u001B[39my_val)\n",
      "File \u001B[1;32m<__array_function__ internals>:200\u001B[0m, in \u001B[0;36msavez\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32mF:\\binbin\\app\\conda\\envs\\Pytorch1\\lib\\site-packages\\numpy\\lib\\npyio.py:615\u001B[0m, in \u001B[0;36msavez\u001B[1;34m(file, *args, **kwds)\u001B[0m\n\u001B[0;32m    531\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_savez_dispatcher)\n\u001B[0;32m    532\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msavez\u001B[39m(file, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[0;32m    533\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Save several arrays into a single file in uncompressed ``.npz`` format.\u001B[39;00m\n\u001B[0;32m    534\u001B[0m \n\u001B[0;32m    535\u001B[0m \u001B[38;5;124;03m    Provide arrays as keyword arguments to store them under the\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    613\u001B[0m \n\u001B[0;32m    614\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 615\u001B[0m     \u001B[43m_savez\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\binbin\\app\\conda\\envs\\Pytorch1\\lib\\site-packages\\numpy\\lib\\npyio.py:712\u001B[0m, in \u001B[0;36m_savez\u001B[1;34m(file, args, kwds, compress, allow_pickle, pickle_kwargs)\u001B[0m\n\u001B[0;32m    709\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    710\u001B[0m     compression \u001B[38;5;241m=\u001B[39m zipfile\u001B[38;5;241m.\u001B[39mZIP_STORED\n\u001B[1;32m--> 712\u001B[0m zipf \u001B[38;5;241m=\u001B[39m \u001B[43mzipfile_factory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mw\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    714\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, val \u001B[38;5;129;01min\u001B[39;00m namedict\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    715\u001B[0m     fname \u001B[38;5;241m=\u001B[39m key \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.npy\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[1;32mF:\\binbin\\app\\conda\\envs\\Pytorch1\\lib\\site-packages\\numpy\\lib\\npyio.py:103\u001B[0m, in \u001B[0;36mzipfile_factory\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mzipfile\u001B[39;00m\n\u001B[0;32m    102\u001B[0m kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mallowZip64\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 103\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mzipfile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mZipFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\binbin\\app\\conda\\envs\\Pytorch1\\lib\\zipfile.py:1253\u001B[0m, in \u001B[0;36mZipFile.__init__\u001B[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001B[0m\n\u001B[0;32m   1251\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m   1252\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1253\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp \u001B[38;5;241m=\u001B[39m \u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilemode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1254\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[0;32m   1255\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m filemode \u001B[38;5;129;01min\u001B[39;00m modeDict:\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../imbd/imdb_train.npz'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bb22d6d1db13fc0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T03:24:38.869870Z",
     "start_time": "2024-12-31T03:24:31.268532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 加载IMDB数据集\n",
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words=10000)\n",
    "\n",
    "# 将整数标签转换为单词文本\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# 创建一个反向映射词典\n",
    "index_word = {index: word for word, index in word_index.items()}\n",
    "\n",
    "# 将电影评论的数字转为单词\n",
    "train_texts = [' '.join([index_word.get(i - 3, '?') for i in review]) for review in train_data]\n",
    "test_texts = [' '.join([index_word.get(i - 3, '?') for i in review]) for review in test_data]\n",
    "\n",
    "# 合并训练和测试数据\n",
    "texts = train_texts + test_texts"
   ],
   "id": "e68ac5b59e8c9953",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-31T03:24:44.201757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm  # 导入 tqdm\n",
    "\n",
    "# 下载 NLTK 停用词库\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 获取英文停用词\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 定义文本清理函数\n",
    "def clean_text(text):\n",
    "    # 转小写\n",
    "    text = text.lower()\n",
    "\n",
    "    # 去除标点符号和非字母字符\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    # 按空格分词\n",
    "    words = text.split()\n",
    "\n",
    "    # 去除停用词\n",
    "    cleaned_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # 重新组合为字符串\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    return cleaned_text\n",
    "\n",
    "# 使用 tqdm 显示进度条，对所有文本应用清理函数\n",
    "cleaned_texts = []\n",
    "for text in tqdm(texts, desc=\"Cleaning Texts\", unit=\"text\"):\n",
    "    cleaned_texts.append(clean_text(text))\n"
   ],
   "id": "1515a1e0880821ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T13:18:47.936385Z",
     "start_time": "2024-12-26T13:18:47.921373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({\n",
    "    'Text': texts[:10],  \n",
    "})\n",
    "# 显示表格\n",
    "df1[0]"
   ],
   "id": "606d8d76c97511cf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                Text\n",
       "0  ? this film was just brilliant casting locatio...\n",
       "1  ? big hair big boobs bad music and a giant saf...\n",
       "2  ? this has to be one of the worst films of the...\n",
       "3  ? the ? ? at storytelling the traditional sort...\n",
       "4  ? worst mistake of my life br br i picked this..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>? this film was just brilliant casting locatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>? big hair big boobs bad music and a giant saf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>? this has to be one of the worst films of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>? the ? ? at storytelling the traditional sort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>? worst mistake of my life br br i picked this...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T13:18:34.532046Z",
     "start_time": "2024-12-26T13:18:34.513510Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                Text\n",
       "0  film brilliant casting location scenery story ...\n",
       "1  big hair big boobs bad music giant safety pin ...\n",
       "2  one worst films friends watching film target a...\n",
       "3  storytelling traditional sort many years event...\n",
       "4  worst mistake life br br picked movie target f..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>film brilliant casting location scenery story ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>big hair big boobs bad music giant safety pin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one worst films friends watching film target a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>storytelling traditional sort many years event...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worst mistake life br br picked movie target f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8,
   "source": [
    "df2 = pd.DataFrame({\n",
    "    'Text': cleaned_texts[:10],  \n",
    "})\n",
    "# 显示表格\n",
    "df2.head()"
   ],
   "id": "3bb3c156b2784f9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T13:28:36.267647Z",
     "start_time": "2024-12-26T13:28:23.438392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "# 训练词向量\n",
    "model = Word2Vec(LineSentence(\"cleaned_texts.txt\"), vector_size=50, window=5, min_count=1, workers=5, sg=0)\n",
    "\n",
    "# 保存模型\n",
    "model.save(\"word2vec_50dim_large.model\")\n",
    "\n",
    "# 将词向量保存为 txt 文件\n",
    "with open(\"txt/word2vec_vectors_50d.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    # 写入词汇表的大小和维度\n",
    "    f.write(f\"{len(model.wv.index_to_key)} {model.vector_size}\\n\")\n",
    "    \n",
    "    # 遍历词向量并写入文件\n",
    "    for word in model.wv.index_to_key:\n",
    "        vector = model.wv[word]\n",
    "        vector_str = \" \".join(map(str, vector))  # 将向量转换为字符串\n",
    "        f.write(f\"{word} {vector_str}\\n\")\n"
   ],
   "id": "e1268be8b5f2b402",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T13:29:18.473385Z",
     "start_time": "2024-12-26T13:29:18.389833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载已经训练好的模型\n",
    "model = Word2Vec.load(\"word2vec_50dim_large.model\")\n",
    "\n",
    "# 获取词向量\n",
    "vector = model.wv['learning']\n",
    "print(f\"Vector for 'learning':\\n\", vector)\n",
    "\n",
    "# 查找与指定词相似的词\n",
    "similar = model.wv.most_similar('learning', topn=5)\n",
    "print(f\"Words most similar to 'learning':\\n\", similar)\n"
   ],
   "id": "2c35ee99434b2b2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'machine':\n",
      " [-1.25276685e+00 -4.51283097e-01 -3.36615205e-01 -6.01255715e-01\n",
      " -4.40078348e-01  5.31646550e-01 -1.84708416e-01 -1.30145311e-01\n",
      "  1.09507535e-02  3.73301119e-01 -9.03674662e-02 -1.12248409e+00\n",
      "  5.69665492e-01 -7.67398715e-01 -5.39114833e-01  3.85416210e-01\n",
      " -1.18252695e-01 -1.13752866e+00  6.59926757e-02 -1.28904128e+00\n",
      " -7.75832683e-02 -1.23484731e-01 -2.81328976e-01  5.52245140e-01\n",
      " -5.69858193e-01  1.26973704e-01 -2.01457962e-01 -4.28240836e-01\n",
      "  6.98231459e-01 -5.16424537e-01  1.03311682e+00 -4.56775427e-01\n",
      "  6.63869262e-01  6.42004251e-01 -8.02595913e-01  1.35936129e+00\n",
      "  5.69961250e-01  1.01413047e+00 -1.15683176e-01 -8.61742126e-04\n",
      "  3.73278886e-01 -4.19324607e-01 -7.40183443e-02  1.19746542e+00\n",
      "  4.65165675e-01  1.39669582e-01  1.05144703e+00 -7.96834946e-01\n",
      " -2.02922747e-01  2.02567413e-01]\n",
      "Words most similar to 'machine':\n",
      " [('taught', 0.7358053922653198), ('teaching', 0.7238261699676514), ('education', 0.701050877571106), ('accepted', 0.6917518973350525), ('teaches', 0.6865631341934204)]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7913b092f0c33135"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
